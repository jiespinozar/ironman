{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99e63a42",
   "metadata": {},
   "source": [
    "# Ironman Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b6cc59",
   "metadata": {},
   "source": [
    "Ironman can perform joint fits of the photometry, out-of-transit radial velocities, and Rossiter-McLaughlin effect for a given exoplanet system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67305ced",
   "metadata": {},
   "source": [
    "First, we need to import our data properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517681ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ironman1\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16389b9",
   "metadata": {},
   "source": [
    "We can create a function to read our different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e24b47ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(archivo):\n",
    "    file = open(archivo,\"r\")\n",
    "    bjd = []\n",
    "    rv = []\n",
    "    e_rv = []\n",
    "    for linea in file:\n",
    "        if linea[0]!=\"#\":\n",
    "            linea = linea.split()\n",
    "            bjd.append(float(linea[0]))\n",
    "            rv.append(float(linea[1]))\n",
    "            e_rv.append(float(linea[2]))\n",
    "            #print(linea)\n",
    "    bjd = np.array(bjd)\n",
    "    rv = np.array(rv)\n",
    "    e_rv = np.array(e_rv)\n",
    "    return bjd,rv,e_rv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a7350",
   "metadata": {},
   "source": [
    "Then, we can use the function to read the radial velocities (RVs) of different instruments. All RVs must be in m/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c8db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bjd_chiron, rv_chiron, e_rv_chiron = read_file(\"Example_Data/chiron_rvs.txt\")\n",
    "\n",
    "bjd_mine, rv_mine, e_rv_mine = read_file(\"Example_Data/minerva_rvs.txt\")\n",
    "\n",
    "bjd_feros, rv_feros, e_rv_feros = read_file(\"Example_Data/feros_rvs.txt\")\n",
    "rv_feros = rv_feros*1000.0\n",
    "e_rv_feros = e_rv_feros*1000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a41a95",
   "metadata": {},
   "source": [
    "Next, we need to import our photometric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "515ad3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tess1 = pd.read_csv(\"Example_Data/TESS_LC_detrended_Y1.csv\")\n",
    "bjd_tess1 = tess1.time.values\n",
    "flux_tess1 = tess1.flux.values\n",
    "e_flux_tess1 = tess1.flux_err.values\n",
    "\n",
    "tess2 = pd.read_csv(\"Example_Data/TESS_LC_detrended_Y2.csv\")\n",
    "bjd_tess2 = tess2.time.values\n",
    "flux_tess2 = tess2.flux.values\n",
    "e_flux_tess2 = tess2.flux_err.values\n",
    "\n",
    "tess3 = pd.read_csv(\"Example_Data/TESS_LC_detrended_Y3.csv\")\n",
    "bjd_tess3 = tess3.time.values\n",
    "flux_tess3 = tess3.flux.values\n",
    "e_flux_tess3 = tess3.flux_err.values\n",
    "\n",
    "FITO = pd.read_csv(\"Example_Data/FITO_det.csv\")\n",
    "bjd_fito = FITO.time.values\n",
    "flux_fito = FITO.flux.values\n",
    "flux_err_fito = FITO.flux_err.values\n",
    "\n",
    "B = pd.read_csv(\"Example_Data/LCOGT_B_det.csv\")\n",
    "bjd_LCOGT_B = B.time.values\n",
    "flux_LCOGT_B = B.flux.values\n",
    "flux_err_LCOGT_B = B.flux_err.values\n",
    "\n",
    "zs = pd.read_csv(\"Example_Data/LCOGT_zs_det.csv\")\n",
    "bjd_LCOGT_zs = zs.time.values\n",
    "flux_LCOGT_zs = zs.flux.values\n",
    "flux_err_LCOGT_zs = zs.flux_err.values\n",
    "\n",
    "A1 = pd.read_csv(\"Example_Data/ASTEP1_det.csv\")\n",
    "bjd_astep1 = A1.time.values\n",
    "flux_astep1 = A1.flux.values\n",
    "flux_err_astep1 = A1.flux_err.values\n",
    "\n",
    "A2 = pd.read_csv(\"Example_Data/ASTEP2_det.csv\")\n",
    "bjd_astep2 = A2.time.values\n",
    "flux_astep2 = A2.flux.values\n",
    "flux_err_astep2 = A2.flux_err.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cfbb2d",
   "metadata": {},
   "source": [
    "And finally the RM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "163530f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bjd_RM, rv_RM, e_rv_RM = read_file(\"Example_Data/TOI3362_RVs.ascii\")\n",
    "rv_RM = rv_RM*1000.0\n",
    "e_rv_RM = e_rv_RM*1000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6575a",
   "metadata": {},
   "source": [
    "We need to create 9 dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84ad231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_lc, fluxes, fluxes_error = {}, {}, {}\n",
    "times_rvs, rvs, rvs_err =  {}, {}, {}\n",
    "times_RM, RM, RM_err =  {}, {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fd3e82",
   "metadata": {},
   "source": [
    "And then add our data to the respective dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ab1b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_rvs[\"CHIRON\"], rvs[\"CHIRON\"], rvs_err[\"CHIRON\"] = bjd_chiron, rv_chiron, e_rv_chiron\n",
    "times_rvs[\"FEROS\"], rvs[\"FEROS\"], rvs_err[\"FEROS\"] = bjd_feros, rv_feros, e_rv_feros\n",
    "times_rvs[\"MINE\"], rvs[\"MINE\"], rvs_err[\"MINE\"] = bjd_mine, rv_mine, e_rv_mine\n",
    "\n",
    "times_lc[\"FITO\"], fluxes[\"FITO\"], fluxes_error[\"FITO\"] = bjd_fito, flux_fito, flux_err_fito\n",
    "times_lc[\"TESSY1\"], fluxes[\"TESSY1\"], fluxes_error[\"TESSY1\"] = bjd_tess1, flux_tess1, e_flux_tess1\n",
    "times_lc[\"TESSY2\"], fluxes[\"TESSY2\"], fluxes_error[\"TESSY2\"] = bjd_tess2, flux_tess2, e_flux_tess2\n",
    "times_lc[\"TESSY3\"], fluxes[\"TESSY3\"], fluxes_error[\"TESSY3\"] = bjd_tess3, flux_tess3, e_flux_tess3\n",
    "times_lc[\"B\"], fluxes[\"B\"], fluxes_error[\"B\"] = bjd_LCOGT_B, flux_LCOGT_B, flux_err_LCOGT_B\n",
    "times_lc[\"ZS\"], fluxes[\"ZS\"], fluxes_error[\"ZS\"] = bjd_LCOGT_zs, flux_LCOGT_zs, flux_err_LCOGT_zs\n",
    "times_lc[\"ASTEP\"], fluxes[\"ASTEP\"], fluxes_error[\"ASTEP\"] = np.concatenate((bjd_astep1,bjd_astep2)), np.concatenate((flux_astep1,flux_astep2)), np.concatenate((flux_err_astep1,flux_err_astep2))\n",
    "\n",
    "times_RM[\"ESPRESSO\"], RM[\"ESPRESSO\"], RM_err[\"ESPRESSO\"] = bjd_RM, rv_RM, e_rv_RM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520cb04a",
   "metadata": {},
   "source": [
    "For the photometric and RM data, we need to create an extra dictionary containing the exposure times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0be1b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = {\"FITO\": False, \"ESPRESSO\": 310./60./60./24., \"TESSY1\": 30./60./24., \"TESSY2\": 10./60./24., \"TESSY3\": False, \"B\": False, \"ZS\": False, \"ASTEP\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5f38e",
   "metadata": {},
   "source": [
    "Then, we have to add those dictionaries to the Ironman data organizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab272ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading LCs...\n",
      "Reading RVs...\n",
      "Reading RM data...\n"
     ]
    }
   ],
   "source": [
    "my_data = ironman1.Data_Org(lc_time=times_lc,lc_flux=fluxes,lc_flux_err=fluxes_error,rv_time=times_rvs,rv=rvs,rv_err=rvs_err,rm_time=times_RM,rm=RM,rm_err=RM_err,verbose = True,exp_times=exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5710fe73",
   "metadata": {},
   "source": [
    "Also, we need to add our priors. This function needs a Data_Org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "037b2b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priors dictionary ready...\n",
      "Detecting fixed parameters...\n"
     ]
    }
   ],
   "source": [
    "my_priors = ironman1.Priors(\"priors_example.dat\",my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44d435",
   "metadata": {},
   "source": [
    "Finally, using these two previous objects, we create our Fit object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3083e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fit = ironman1.Fit(my_data,my_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a5283",
   "metadata": {},
   "source": [
    "To run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a5ef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "post = my_fit.run(n_live=3500, nthreads = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6af768f",
   "metadata": {},
   "source": [
    "The variable post contains the flatchain. Using 3500 n_live and 40 threads it takes ~ 1 hour  to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3405b36",
   "metadata": {},
   "source": [
    "We can save the flatchain using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f1cfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('flatchain.csv', post, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8395f8a",
   "metadata": {},
   "source": [
    "We can analyse it later with another script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce9fbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
